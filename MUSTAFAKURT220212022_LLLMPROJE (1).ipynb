{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip -q install --no-cache-dir datasets transformers evaluate accelerate scikit-learn matplotlib\n"
      ],
      "metadata": {
        "id": "Imqt23sVrqfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
        "                          TrainingArguments, Trainer, DataCollatorWithPadding, set_seed)\n",
        "import evaluate\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "set_seed(42)\n"
      ],
      "metadata": {
        "id": "TaSaWo4gr02n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "def load_turkish_movie_dataset():\n",
        "    candidates = [\n",
        "        \"mkeskin/turkish_movie_sentiment\",\n",
        "    ]\n",
        "    for ds_id in candidates:\n",
        "        try:\n",
        "            raw = load_dataset(ds_id)\n",
        "            print(\"Loaded:\", ds_id)\n",
        "            return ds_id, raw\n",
        "        except Exception as e:\n",
        "            print(f\"Failed: {ds_id} -> {type(e).__name__}: {e}\")\n",
        "\n",
        "    api = HfApi()\n",
        "    results = list(api.list_datasets(search=\"turkish movie sentiment\"))\n",
        "    print(\"Search results:\", len(results))\n",
        "\n",
        "    for r in results[:40]:\n",
        "        ds_id = r.id\n",
        "        try:\n",
        "            raw = load_dataset(ds_id)\n",
        "            if \"train\" in raw:\n",
        "                print(\"Auto-found:\", ds_id)\n",
        "                return ds_id, raw\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    raise RuntimeError(\"Uygun Türkçe film yorum dataset'i bulunamadı. (Gerekirse CSV yükleyip devam ederiz.)\")\n",
        "\n",
        "ds_id, raw = load_turkish_movie_dataset()\n",
        "raw\n"
      ],
      "metadata": {
        "id": "hdkYdF0js96u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_split = raw[\"train\"]\n",
        "cols = train_split.column_names\n",
        "print(\"Columns:\", cols)\n",
        "\n",
        "def infer_columns(ds):\n",
        "    cols = ds.column_names\n",
        "    low = {c.lower(): c for c in cols}\n",
        "\n",
        "    text_candidates = [\"text\", \"review\", \"comment\", \"sentence\", \"content\"]\n",
        "    label_candidates = [\"label\", \"rating\", \"stars\", \"score\", \"point\", \"puan\", \"yildiz\"]\n",
        "\n",
        "    text_col = None\n",
        "    for k in text_candidates:\n",
        "        if k in low:\n",
        "            text_col = low[k]; break\n",
        "\n",
        "    label_col = None\n",
        "    for k in label_candidates:\n",
        "        if k in low:\n",
        "            label_col = low[k]; break\n",
        "\n",
        "    # ilk string kolonu text yapıyoruz\n",
        "    if text_col is None:\n",
        "        for c in cols:\n",
        "            try:\n",
        "                if ds.features[c].dtype == \"string\":\n",
        "                    text_col = c\n",
        "                    break\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    if text_col is None or label_col is None:\n",
        "        raise ValueError(f\"Kolonlar tespit edilemedi. Bulunan text={text_col}, label={label_col}. Kolonlar={cols}\")\n",
        "\n",
        "    return text_col, label_col\n",
        "\n",
        "text_col, label_col = infer_columns(train_split)\n",
        "print(\"Detected text_col:\", text_col, \"| label_col:\", label_col)\n",
        "\n",
        "print(\"Sample text:\", train_split[0][text_col])\n",
        "print(\"Sample label:\", train_split[0][label_col])\n"
      ],
      "metadata": {
        "id": "0pHo6Fa3tB35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "\n",
        "def to_5class_label(x):\n",
        "    #  int ise bunu\n",
        "    if isinstance(x, (int, np.integer)):\n",
        "        if 0 <= x <= 4:\n",
        "            return int(x)\n",
        "        if 1 <= x <= 5:\n",
        "            return int(x - 1)\n",
        "\n",
        "    #  float ise bunu\n",
        "    if isinstance(x, (float, np.floating)):\n",
        "        y = int(round(float(x)))\n",
        "        y = max(1, min(5, y))\n",
        "        return y - 1\n",
        "\n",
        "    # string ise bunu\n",
        "    if isinstance(x, str):\n",
        "        s = x.strip()\n",
        "\n",
        "        s = s.replace(\" \", \"\")\n",
        "\n",
        "        s = s.replace(\",\", \".\")\n",
        "\n",
        "        s = re.sub(r\"[^0-9\\.\\-]\", \"\", s)\n",
        "\n",
        "        if s == \"\" or s == \".\" or s == \"-\":\n",
        "            raise ValueError(f\"Boş/Geçersiz label string: {x!r}\")\n",
        "\n",
        "        xf = float(s)\n",
        "        y = int(round(xf))\n",
        "        y = max(1, min(5, y))\n",
        "        return y - 1\n",
        "\n",
        "\n",
        "    raise ValueError(f\"Label dönüştürülemedi: {x} ({type(x)})\")\n"
      ],
      "metadata": {
        "id": "faPzuhl8tGee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_clean = standardize(train_split, text_col, label_col)\n",
        "ds_clean[0]"
      ],
      "metadata": {
        "id": "ZVFaMsBNtwd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    print(train_split[i][label_col])"
      ],
      "metadata": {
        "id": "JMYLnu5Mtz7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp = ds_clean.to_pandas()\n",
        "print(tmp[\"label\"].value_counts().sort_index())"
      ],
      "metadata": {
        "id": "GXx9GfSpt5uA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = ds_clean.to_pandas()\n",
        "\n",
        "train_df, temp_df = train_test_split(\n",
        "    df, test_size=0.2, random_state=42, stratify=df[\"label\"]\n",
        ")\n",
        "val_df, test_df = train_test_split(\n",
        "    temp_df, test_size=0.5, random_state=42, stratify=temp_df[\"label\"]\n",
        ")\n",
        "\n",
        "dataset = DatasetDict({\n",
        "    \"train\": Dataset.from_pandas(train_df.reset_index(drop=True)),\n",
        "    \"validation\": Dataset.from_pandas(val_df.reset_index(drop=True)),\n",
        "    \"test\": Dataset.from_pandas(test_df.reset_index(drop=True)),\n",
        "})\n",
        "dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "b8nMkzmKt7XP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"dbmdz/bert-base-turkish-cased\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch[\"text\"], truncation=True, max_length=256)\n",
        "\n",
        "tokenized = dataset.map(tokenize, batched=True)\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5)\n"
      ],
      "metadata": {
        "id": "2ok2BtMouArA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = evaluate.load(\"accuracy\")\n",
        "f1 = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    return {\n",
        "        \"accuracy\": acc.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
        "        \"macro_f1\": f1.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"],\n",
        "    }\n"
      ],
      "metadata": {
        "id": "9M-O1wFWuOnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"berturk_movie_sentiment\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"macro_f1\",\n",
        "    greater_is_better=True,\n",
        "    fp16=True,\n",
        "    logging_steps=50,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized[\"train\"],\n",
        "    eval_dataset=tokenized[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "jD9zaCcXuSku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_metrics = trainer.evaluate(tokenized[\"test\"])\n",
        "print(\"TEST METRICS:\", test_metrics)\n",
        "\n",
        "pred = trainer.predict(tokenized[\"test\"])\n",
        "y_true = pred.label_ids\n",
        "y_pred = np.argmax(pred.predictions, axis=-1)\n",
        "\n",
        "print(classification_report(y_true, y_pred, digits=4))"
      ],
      "metadata": {
        "id": "6uFE7MDfWi5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.imshow(cm)\n",
        "plt.title(\"Confusion Matrix (0-4 => stars 1-5)\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.colorbar()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "85g1VUZ-W1PX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_texts = tokenized[\"test\"]\n",
        "wrong_idx = np.where(y_true != y_pred)[0]\n",
        "\n",
        "print(\"Wrong predictions:\", len(wrong_idx), \"out of\", len(y_true))\n",
        "\n",
        "for i in wrong_idx[:10]:\n",
        "    print(\"=\"*80)\n",
        "    print(\"TRUE:\", int(y_true[i]+1), \"| PRED:\", int(y_pred[i]+1))\n",
        "    print(test_texts[int(i)]['text'][:500])"
      ],
      "metadata": {
        "id": "5d52Gz9yW2s4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}